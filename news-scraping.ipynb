{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Scrape RSS Newsfeed\n",
    "\n",
    "In this notebook we are going to scrape the articles from an RSS newsfeed which contains the latest articles posted by CNN across their various subdomains, extract the article information, and store our results in a csv file.\n",
    "\n",
    "<img src=\"./img/CNN_scrape.png\" alt=\"CNN scraping\" width=\"800\"/>\n",
    "\n",
    "Each article result will contain -\n",
    " - Date \n",
    " - Publilsher\n",
    " - Title\n",
    " - Author(s)\n",
    " - Article URL\n",
    " - Content\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Program Parameters\n",
    "\n",
    " - `NEWS_FEEDS` -> This is the list of supported newsfeeds, which may be expanded in the future. Only CNN supported for now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "NEWS_FEEDS = {\n",
    "    'CNN' : 'http://rss.cnn.com/rss/cnn_latest.rss'\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports\n",
    "\n",
    "We are using `requests` and `BeautifulSoup` to request and parse the html, respectively. We are using `datetime` to parse dates into timestamps. Finally, `pandas` is used to export the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import requests\n",
    "from datetime import datetime\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_urls_from_rss(url=str, format=str):\n",
    "    '''Scrape news articles from an RSS feed. `url` is the url of the feed. `format` is the format of the feed.'''\n",
    "    supported_news_feeds = {'CNN'}\n",
    "    if(format not in supported_news_feeds):\n",
    "        raise Exception('Format not recognised.')\n",
    "    html = requests.get(url)\n",
    "    xml = BeautifulSoup(html.text, 'xml')\n",
    "    article_urls = list()\n",
    "    articles = xml.find_all('item')\n",
    "    for article in articles:\n",
    "        article_urls.append(article.find('guid').text)\n",
    "    return article_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_cnn_article(html_text):\n",
    "    '''Method to scrape CNN article from any subdomain'''\n",
    "    article = dict()\n",
    "    \n",
    "    # Title\n",
    "    title = html_text.find('h1', class_='pg-headline')\n",
    "    if(title is None):\n",
    "        title = html_text.find('h1', class_='headline__text')\n",
    "    if(title is None):\n",
    "        title = html_text.find('h1', class_='Article__title')\n",
    "    if(title is None):\n",
    "        title = html_text.find('h1', class_='PageHead__title')\n",
    "    if(title is None):\n",
    "        title = html_text.find('div', 'SpecialArticle__headTitle')\n",
    "\n",
    "    article[\"Title\"] = title.text.strip()\n",
    "\n",
    "    # Authors\n",
    "    author_list = \"\"\n",
    "    i = 0\n",
    "    authors = html_text.find('span', class_= 'metadata__byline__author')\n",
    "    if authors is None:\n",
    "        authors = html_text.find('div', class_= 'Article__subtitle')\n",
    "        if authors is None:\n",
    "            authors = html_text.find_all('span', class_= 'byline__name')\n",
    "            if authors == []:\n",
    "                authors = html_text.find_all('span', class_='Authors__writer')\n",
    "            if authors == []:\n",
    "                authors = html_text.find_all('span', class_='SpecialArticle__writer')\n",
    "            for author in authors:\n",
    "                if(i == 0):\n",
    "                    author_list += author.text\n",
    "                else:\n",
    "                    author_list += \", \" + author.text\n",
    "                i += 1\n",
    "        else:\n",
    "            author_list = authors.text[:authors.text.find('â€¢')-6] \n",
    "    else:\n",
    "        authors = authors.text.split(' ')\n",
    "        print(f'Metadata found: Authors - {authors}')\n",
    "        if authors.__contains__('CNN'):\n",
    "            del(authors[authors.index('CNN'):])\n",
    "        if authors.__contains__('By'):\n",
    "            del(authors[:authors.index('By')+1])\n",
    "        if authors.__contains__('by'):\n",
    "            del(authors[:authors.index('by')+1])\n",
    "        print(f'Trimmed Author List - {authors}')\n",
    "        author_list = \" \".join(authors)\n",
    "    if not author_list == \"\":\n",
    "        if author_list[-1] == ',':\n",
    "            author_list = author_list[:-1]\n",
    "\n",
    "    article['Authors'] = author_list or 'Anonymous'\n",
    "\n",
    "    # Content\n",
    "    content = \"\"\n",
    "    lead_text_ps = html_text.find_all('p', class_='zn-body__paragraph speakable')\n",
    "    if lead_text_ps == []:\n",
    "        paragraphs = html_text.find_all('p', class_='paragraph inline-placeholder')\n",
    "        if paragraphs == []:\n",
    "            paragraphs = html_text.find_all('div', class_='Paragraph__component')\n",
    "            if paragraphs == []:\n",
    "                paragraphs = html_text.find_all('div', class_='SpecialArticle__paragraph')\n",
    "    else:\n",
    "        for p in lead_text_ps:\n",
    "            exclude_text = False\n",
    "            for parent in p.parents:\n",
    "                if parent == 'q':\n",
    "                    exclude_text = True\n",
    "            if not exclude_text:\n",
    "                content = p.text[p.text.find(')')+1:].strip()\n",
    "\n",
    "        paragraphs = html_text.find_all('div', class_='zn-body__paragraph')\n",
    "\n",
    "    for paragraph in paragraphs:\n",
    "        content += ' ' + paragraph.text.strip()\n",
    "\n",
    "    article['Content'] = content.replace('\\n', \"\")\n",
    "\n",
    "    # Date\n",
    "    updated = html_text.find('p', class_='update-time')\n",
    "    if updated is None:\n",
    "        # print('UpdateTime not found') \n",
    "        updated = html_text.find('div', class_='timestamp')\n",
    "        if updated is None:\n",
    "            # print('Timestamp not found')\n",
    "            updated = html_text.find('div', class_='PageHead__published')\n",
    "            if updated is None:\n",
    "                updated = html_text.find('div', class_='SpecialArticle__details')\n",
    "                if updated is None:\n",
    "                    updated = html_text.find('div', class_='Article__subtitle')\n",
    "            date_array = updated.text.strip().split(\" \")[-3:]\n",
    "            date_array[0] = date_array[0][:-2]\n",
    "            if len(date_array[0]) == 1:\n",
    "                date_array[0] = '0' + date_array[0]\n",
    "            date_string = \" \".join(date_array)\n",
    "            date = datetime.strptime(date_string, '%d %B %Y')\n",
    "        else:\n",
    "            date_array = updated.text.strip().split(\" \")[1:]\n",
    "            date_string = \" \".join(date_array).strip()\n",
    "            date_string = \" \".join(date_string.split(\" \")[4:])\n",
    "    else:\n",
    "        # print('UpdateTime found')\n",
    "        date_string = updated.text[updated.text.find(')')+2:].strip()\n",
    "\n",
    "    try:\n",
    "        date\n",
    "    except NameError:\n",
    "        date = datetime.strptime(date_string, '%B %d, %Y') \n",
    "    \n",
    "    article['Date'] = date \n",
    "\n",
    "    return article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_article_from_url(url=str, format=str):\n",
    "    '''Scrape news article from a given url. `url` is the url of the feed. `format` is the format of the publisher's articles.'''\n",
    "    supported_formats = {'CNN'}\n",
    "    if(format not in supported_formats):\n",
    "        raise Exception('Format not recognised.')\n",
    "    \n",
    "    html = requests.get(url)\n",
    "    html_text = BeautifulSoup(html.text, 'lxml')\n",
    "    \n",
    "    # Debug HTML\n",
    "    # print(html_text)\n",
    "\n",
    "    if(format == 'CNN'):\n",
    "        if not url.__contains__('/live-news/'):\n",
    "            article = scrape_cnn_article(html_text)\n",
    "        else:\n",
    "            article = None\n",
    "            print('/live-news/ page skipped.\\n')\n",
    "    elif(format == 'Fox'):\n",
    "        pass\n",
    "    elif(format == 'NPR'):\n",
    "        pass\n",
    "    else:\n",
    "        raise Exception('Format not recognised.')\n",
    "\n",
    "    if article is None:\n",
    "        return\n",
    "    else:\n",
    "        # Publisher\n",
    "        article[\"Publisher\"] = format\n",
    "        # URL\n",
    "        article['URL'] = url\n",
    "        return article"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_articles_from_feed(url=str, format=str):\n",
    "    '''Scrape news articles from an RSS feed. `url` is the url of the feed. `format` is the format of the articles.'''\n",
    "    results = list()\n",
    "\n",
    "    for url in scrape_urls_from_rss(url, format):\n",
    "        print(f'Scraping URL => {url}')\n",
    "        article = scrape_article_from_url(url, format)\n",
    "        if article is not None:\n",
    "            print('|\\n|==> Scraped: ' + article['Title'] + '\\n')\n",
    "            results.append(article)\n",
    "    \n",
    "    return results\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scraping Newsfeed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Scraping URL => https://www.cnn.com/2022/10/05/world/spacex-nasa-crew-5-astronaut-launch-scn/index.html\n",
      "|\n",
      "|==> Scraped: SpaceX, NASA launch 3 astronauts and 1 cosmonaut to the ISS. Hereâ€™s everything you need to know\n",
      "\n",
      "Scraping URL => https://www.cnn.com/2022/10/05/football/alex-ferguson-jose-mourinho-english-dictionary-spt-intl/index.html\n",
      "|\n",
      "|==> Scraped: Soccer lexicon: â€˜Squeaky bum timeâ€™ and â€˜park the busâ€™ added to Oxford English Dictionary\n",
      "\n",
      "Scraping URL => https://www.cnn.com/2022/10/05/uk/liz-truss-greenpeace-conservative-conference-gbr-intl/index.html\n",
      "|\n",
      "|==> Scraped: Greenpeace campaigners disrupt Liz Trussâ€™s party conference speech\n",
      "\n",
      "Scraping URL => https://www.cnn.com/2022/10/05/us/hurricane-ian-florida-recovery-wednesday/index.html\n",
      "|\n",
      "|==> Scraped: Sanibel Island residents return to see if their homes survived devastating Hurricane Ian\n",
      "\n",
      "Scraping URL => https://www.cnn.com/2022/10/05/us/california-family-missing-wednesday/index.html\n",
      "|\n",
      "|==> Scraped: Search continues for abducted California family as relatives appeal for publicâ€™s help in finding them\n",
      "\n",
      "Scraping URL => https://www.cnn.com/2022/10/05/sport/home-run-ball-value-aaron-judge-spt-intl/index.html\n",
      "|\n",
      "|==> Scraped: Aaron Judgeâ€™s 62nd home run ball is valued in the region of $1-2 million, experts say\n",
      "\n",
      "Scraping URL => https://www.cnn.com/2022/09/29/investing/premarket-trading-stocks/index.html\n",
      "|\n",
      "|==> Scraped: The bond market is crumbling. Thatâ€™s bad for Wall Street and Main Street\n",
      "\n",
      "Scraping URL => https://www.cnn.com/2022/10/05/politics/herschel-walker-georgia-senate-what-comes-next/index.html\n",
      "|\n",
      "|==> Scraped: What is Herschel Walker going to do now?\n",
      "\n",
      "Scraping URL => https://www.cnn.com/world/live-news/spacex-launch-nasa-mission-10-05-22/index.html\n",
      "/live-news/ page skipped.\n",
      "\n",
      "Scraping URL => https://www.cnn.com/2022/10/05/politics/biden-opec-oil/index.html\n",
      "|\n",
      "|==> Scraped: White House says Bidenâ€™s Saudi trip wasnâ€™t a waste as he lambastes OPEC+â€™s â€˜shortsightedâ€™ decision to cut oil output\n",
      "\n",
      "Scraping URL => https://www.cnn.com/2022/10/05/energy/opec-production-cuts/index.html\n",
      "|\n",
      "|==> Scraped: OPEC announces the biggest cut to oil production since the start of the pandemic\n",
      "\n",
      "Scraping URL => https://www.cnn.com/2022/10/05/us/latino-media-underrepresentation-goa-report-reaj/index.html\n",
      "|\n",
      "|==> Scraped: Latino representation in media industry grew by only 1% in the past decade, a new report finds\n",
      "\n",
      "Scraping URL => https://www.cnn.com/2022/10/05/europe/miss-crimea-fined-ukrainian-song-intl/index.html\n",
      "|\n",
      "|==> Scraped: Crimean beauty queen fined by Russian authorities for singing patriotic Ukrainian song\n",
      "\n",
      "Scraping URL => https://www.cnn.com/2022/10/05/entertainment/kourtney-kardashian-khlo-kardashian/index.html\n",
      "|\n",
      "|==> Scraped: Kourtney Kardashian says sheâ€™s not as close to KhloÃ© as she once was\n",
      "\n",
      "Scraping URL => https://www.cnn.com/2022/10/05/entertainment/adam-sandler-interview/index.html\n",
      "|\n",
      "|==> Scraped: The perpetually youthful Adam Sandler talks getting older\n",
      "\n",
      "Scraping URL => https://www.cnn.com/2022/10/05/investing/dow-stock-market-today/index.html\n",
      "|\n",
      "|==> Scraped: Stocks slip modestly Wednesday following huge two-day surge\n",
      "\n",
      "Scraping URL => https://www.cnn.com/2022/10/04/politics/white-house-lobby-opec-oil-production-cuts-gasoline-prices-midterms/index.html\n",
      "|\n",
      "|==> Scraped: Inside the White Houseâ€™s failed effort to dissuade OPEC from cutting oil production to avoid a â€˜total disasterâ€™\n",
      "\n",
      "Scraping URL => https://www.cnn.com/2022/10/05/us/skeleton-home-depot-skelly-halloween-decoration-trend-cec/index.html\n",
      "|\n",
      "|==> Scraped: How a 12-foot skeleton became the hottest Halloween decoration around\n",
      "\n",
      "Scraping URL => https://www.cnn.com/2022/10/05/health/cnn-kff-mental-health-poll-wellness/index.html\n",
      "|\n",
      "|==> Scraped: 90% of US adults say the United States is experiencing a mental health crisis, CNN/KFF poll finds\n",
      "\n",
      "Scraping URL => https://www.cnn.com/2022/10/05/entertainment/coldplay-chris-martin-lung-infection/index.html\n",
      "|\n",
      "|==> Scraped: Chris Martinâ€™s lung infection forces Coldplay to postpone shows\n",
      "\n",
      "Scraping URL => https://www.cnn.com/travel/article/world-best-bars-2022/index.html\n",
      "|\n",
      "|==> Scraped: The world's best bars for 2022 have been revealed\n",
      "\n",
      "Scraping URL => https://www.cnn.com/2022/10/05/us/fbi-national-crime-report-2021-data/index.html\n",
      "|\n",
      "|==> Scraped: The FBI released its crime report for 2021 - but it tells us less about the overall state of crime in the US than ever\n",
      "\n",
      "Scraping URL => https://www.cnn.com/2022/10/05/business/rei-black-friday/index.html\n",
      "|\n",
      "|==> Scraped: REI dumps Black Friday â€” permanently\n",
      "\n",
      "Scraping URL => https://www.cnn.com/2022/10/05/europe/ukraine-conflict-russia-losses-intl/index.html\n",
      "|\n",
      "|==> Scraped: Russia concedes big losses in south as pro-Putin voices paint a grim picture of setbacks\n",
      "\n",
      "Scraping URL => https://www.cnn.com/2022/10/05/middleeast/social-media-disinformation-mime-intl/index.html\n",
      "|\n",
      "|==> Scraped: The battle of narratives on Iran is being fought on social media\n",
      "\n",
      "Scraping URL => https://www.cnn.com/2022/10/05/entertainment/velma-scooby-doo-gay/index.html\n",
      "|\n",
      "|==> Scraped: Velma in new â€˜Scooby Dooâ€™ clip delights fans who say her LGBTQ+ identity has been confirmed\n",
      "\n",
      "Scraping URL => https://www.cnn.com/2013/07/13/world/americas/hurricane-sandy-fast-facts/index.html\n",
      "|\n",
      "|==> Scraped: Hurricane Sandy Fast Facts\n",
      "\n",
      "Scraping URL => https://www.cnn.com/2022/10/05/politics/white-house-student-loan-borrowers-scam/index.html\n",
      "|\n",
      "|==> Scraped: Biden administration working to crack down on scams ahead of student loan forgiveness application process\n",
      "\n",
      "Scraping URL => https://www.cnn.com/2022/10/05/sport/aaron-judge-fan-home-run-spt-intl/index.html\n",
      "|\n",
      "|==> Scraped: Fan who caught Aaron Judge historic home run not sure what he will do with it\n",
      "\n",
      "Scraping URL => https://www.cnn.com/2022/10/05/entertainment/nia-long-ime-udoka/index.html\n",
      "|\n",
      "|==> Scraped: Nia Long shares a tip for protecting mental health\n",
      "\n"
     ]
    }
   ],
   "source": [
    "## Test scrape Newsfeed\n",
    "source = 'CNN'\n",
    "results = scrape_articles_from_feed(url=NEWS_FEEDS[source], format=source)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Date</th>\n",
       "      <th>Publisher</th>\n",
       "      <th>Title</th>\n",
       "      <th>Authors</th>\n",
       "      <th>URL</th>\n",
       "      <th>Content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2022-10-05</td>\n",
       "      <td>CNN</td>\n",
       "      <td>SpaceX, NASA launch 3 astronauts and 1 cosmona...</td>\n",
       "      <td>Jackie Wattles</td>\n",
       "      <td>https://www.cnn.com/2022/10/05/world/spacex-na...</td>\n",
       "      <td>SpaceX and NASA launched a crew of astronauts...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2022-10-05</td>\n",
       "      <td>CNN</td>\n",
       "      <td>Soccer lexicon: â€˜Squeaky bum timeâ€™ and â€˜park t...</td>\n",
       "      <td>Alasdair Howorth</td>\n",
       "      <td>https://www.cnn.com/2022/10/05/football/alex-f...</td>\n",
       "      <td>Soccerâ€™s lexicon is a rich reservoir of often...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2022-10-05</td>\n",
       "      <td>CNN</td>\n",
       "      <td>Greenpeace campaigners disrupt Liz Trussâ€™s par...</td>\n",
       "      <td>Peter Wilkinson, Chris Liakos</td>\n",
       "      <td>https://www.cnn.com/2022/10/05/uk/liz-truss-gr...</td>\n",
       "      <td>Protesters from the environmental group Green...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2022-10-05</td>\n",
       "      <td>CNN</td>\n",
       "      <td>Sanibel Island residents return to see if thei...</td>\n",
       "      <td>Nouran Salahieh, Dakin Andone</td>\n",
       "      <td>https://www.cnn.com/2022/10/05/us/hurricane-ia...</td>\n",
       "      <td>Residents of Floridaâ€™s Sanibel Island are war...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2022-10-05</td>\n",
       "      <td>CNN</td>\n",
       "      <td>Search continues for abducted California famil...</td>\n",
       "      <td>Aya Elamroussi, Natasha Chen, Jack Hannah</td>\n",
       "      <td>https://www.cnn.com/2022/10/05/us/california-f...</td>\n",
       "      <td>The search for a family of four kidnapped in ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Date Publisher                                              Title  \\\n",
       "0 2022-10-05       CNN  SpaceX, NASA launch 3 astronauts and 1 cosmona...   \n",
       "1 2022-10-05       CNN  Soccer lexicon: â€˜Squeaky bum timeâ€™ and â€˜park t...   \n",
       "2 2022-10-05       CNN  Greenpeace campaigners disrupt Liz Trussâ€™s par...   \n",
       "3 2022-10-05       CNN  Sanibel Island residents return to see if thei...   \n",
       "4 2022-10-05       CNN  Search continues for abducted California famil...   \n",
       "\n",
       "                                     Authors  \\\n",
       "0                             Jackie Wattles   \n",
       "1                           Alasdair Howorth   \n",
       "2              Peter Wilkinson, Chris Liakos   \n",
       "3              Nouran Salahieh, Dakin Andone   \n",
       "4  Aya Elamroussi, Natasha Chen, Jack Hannah   \n",
       "\n",
       "                                                 URL  \\\n",
       "0  https://www.cnn.com/2022/10/05/world/spacex-na...   \n",
       "1  https://www.cnn.com/2022/10/05/football/alex-f...   \n",
       "2  https://www.cnn.com/2022/10/05/uk/liz-truss-gr...   \n",
       "3  https://www.cnn.com/2022/10/05/us/hurricane-ia...   \n",
       "4  https://www.cnn.com/2022/10/05/us/california-f...   \n",
       "\n",
       "                                             Content  \n",
       "0   SpaceX and NASA launched a crew of astronauts...  \n",
       "1   Soccerâ€™s lexicon is a rich reservoir of often...  \n",
       "2   Protesters from the environmental group Green...  \n",
       "3   Residents of Floridaâ€™s Sanibel Island are war...  \n",
       "4   The search for a family of four kidnapped in ...  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create dataframe\n",
    "df = pd.DataFrame.from_dict(results)\n",
    "df = df.reindex(columns=[\n",
    "        'Date',\n",
    "        'Publisher',\n",
    "        'Title',\n",
    "        'Authors',\n",
    "        'URL',\n",
    "        'Content',\n",
    "    ])\n",
    "df.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "timestamp_string = str(int(datetime.timestamp(datetime.now())))\n",
    "df.to_csv(path_or_buf=f'outputs/{source}_{timestamp_string}.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.9.13 ('web-scraping')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e9a7d948b703bc7eaf693fa5a4315067dde8a1dcc0809ad0f7482a5f4a995ac5"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
